{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2d3b17-1101-4ebd-89b1-daf665b2cf44",
   "metadata": {},
   "source": [
    "<h3>Carrega bibliotecas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2708b98-5c21-4afa-af04-d6e9295a024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 15:58:01.131447: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-04 15:58:01.175081: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-04 15:58:01.978856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import text_dataset_from_directory\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Input, TextVectorization, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acfd90-4f6a-4bfd-b760-1ca9b0e34328",
   "metadata": {},
   "source": [
    "<h3>Lê os dados dos diretório</h3>\n",
    "A estrutura do diretório deve ser, de acordo com a documentação do Keras:\n",
    "<pre>\n",
    "main_directory/\n",
    "...class_a/\n",
    "......a_text_1.txt\n",
    "......a_text_2.txt\n",
    "...class_b/\n",
    "......b_text_1.txt\n",
    "......b_text_2.txt\n",
    "</pre>\n",
    "Ver documentação <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text_dataset_from_directory\"> aqui </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72cd52f7-cdd9-4819-8eaa-2a3d19a50636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label), # os arquivos possuem quebra de linha\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4c0d2-8853-4296-a39a-ae3694362da1",
   "metadata": {},
   "source": [
    "Esses dados são baseados em review de filmes do site IMDB e formam um banco de dados disponível originalmente em http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c34ee1-960c-48ec-bd59-62e660a0c130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "datadir = \"imdb\"\n",
    "data_train = readData(datadir+\"/train\")\n",
    "data_test  = readData(datadir+\"/test\")\n",
    "\n",
    "text_train = data_train.map(lambda text, label: text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee98b5-8060-43a1-bfb6-a9e6f78af745",
   "metadata": {},
   "source": [
    "<h3>Vamos ver uns dados</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1199093c-f614-4a80-a4a8-04acf9124e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Dan, the widowed father of three girls, has his own advice column that will probably go into syndication. After his wife\\'s death, he has taken time to raise his daughters. Having known no romance in quite some time, nothing prepares him for the encounter with the radiant Marie, at a local book store in a Rhode Island small town on the ocean, where he has gone to celebrate Thanksgiving with the rest of his big family. After liking Marie at first sight, little prepares him when the gorgeous woman appears at the family compound. After all, she is the date of Dan\\'s brother, Mitch.  It is clear from the outset that Dan and Marie are made for one another, and although we sense what the outcome will be, we go for the fun ride that Peter Hedges, the director wants to give us. Mr. Hedges, an author and screenplay writer on his own, has given us two excellent novels, \"What\\'s Eating Gilber Grapes\", and \"An Ocean in Iowa\", and the delightful indie, \"Pieces of April, which he also directed. It\\'s just a coincidence that both movies deal with families during Thanksgiving reunions.  The best thing in the film was the natural chemistry between the two stars, Steve Carell and Juliette Binoche. Mr. Carell, in fact, keeps getting better all the time. In many ways, he remind us of Jack Lemmon, in his take of comedy and serious material. What can one say about Ms. Binoche, an intelligent actress, and a bright presence in any film. She proves she is right up to doing comedy, convincing us about her Marie.  The only sad note is the waste of talent in the picture. John Mahoney, Diane Wiest, Norbert Leo Butz, Jessica Hecht, Emily Blunt, Allison Pill, Amy Ryan, have nothing to do. They just serve as incidental music for decoration. Dane Cook, who is seen as brother Mitch, fares better because he gets to recite more lines than the others.  \"Dan in Real Life\" is a delightful film that will please everyone.'\n",
      "1\n",
      "b'My goodness is this movie bad. I was totally misled by my local movie review because this is certifiable garbage. Yeah, yeah, good guys wear white, bad guys wear black....and the good guys always win. Now go home and hug your kids, and feel how good Hollywood has made you feel. Blech! I can\\'t believe this brain dead movie was made by Wes Craven. I\\'m guessing he needed a little money to pay the mortgage, so he made this piece of dung. It is the sort of production that makes anyone who watches movies regularly believe they could do as good or better than such an experienced director.  Ya see, a bad guy wants a sweet girl who loves her daddy to do a wittle IL\\' bad thing or he\\'s gonna hurt her daddy. But being Ms. All-American girl next door, we know she\\'s gonna save the day and beat the bad guys...the end. Girl power ROCKS.  C\\'mon now, only an idiot would find this entertaining...\"a roller coaster ride,\" let alone something fresh or new. All those \"super-duper\" reviews you see on this site are from industry hacks who are either making money off this flick, paying back a favor, or they have sold their souls to the devil.  Rachael McAdams is beautiful....yup, that\\'s it. Not a good performance, not a horrible one...she\\'s just cute. She would have had to show a whole lot of skin to save this movie. She isn\\'t tough enough to be a good female action lead.  Cillian Murphy was at least passable in 28 Days. But here he plays a dumb villain pretending to be a smart one. He gets his ass kicked to and fro by the 5\\'5\\'\\' McAdams, because after all, she was a cheerleader...and a field hockey player...and I\\'m sure she owns all the Tae Bo tapes...so she should be able to kick the crap out of an international terrorist for hire. I wouldn\\'t trust him to steal a pack of gum from 7-11.  Ya see, this movie was done before, except before they did it well. Go re-rent any of the Die Hard movies. You have loved ones in danger, international terrorists, except the characters are more likable and believable and the bad guys are WAY more competent and interesting. I simply don\\'t understand how Hollywood can continue to make such crap as if they were oblivious to the proper models they can readily copy. No wonder movie revenues are down.  Throw your $6 down the toilet and save yourself 2 hours of your life you\\'ll never get back.  ciao, FreddyShoop'\n",
      "0\n",
      "b'The first half of this movie is a pure delight. Novel. Funny. Wonderful performances. A close knit brother and sister living in Manhattan fall for the same woman! Adult. Bright. Witty. What more could you ask. As a romantic comedy this starts refreshing. It heads into unexplored territory. And then it falls apart.  It goes from being a universal adult comedy to a coming-of-age coming-out-of-the-closet story that has been done many times before. What a disappointment. As a people film it begins with such promise. Why does it need to turn into such a pedestrian \"I am who I am\" film. The freeze-frame ending shot of Heather Graham\\'s jumping in the air to celebrate \"her happiness at finding herself\" underlines the banality of the last part of the film.  It could have been different. It could have been magical. It ended up being the same old same old.'\n",
      "0\n",
      "b'This is strictly a review of the pilot episode as it appears on DVD.  Television moved out of my life in 1981, so I never followed the series or any part of it - which means that I\\'m immune to the nostalgic charm that Moonlighting appears to have for most reviewers.   (Possible spoiler warning)   The pilot of Moonlighting is your basic \"caveman meets fluffball\" yarn, where a \"charming\" red-blooded he-man manipulates a misguided woman into realizing what she really wants and needs. The premises that the script\\'s \"wit\" is based on must have already felt stale around 1950. It also contains some frankly bad writing, as in the scene where Maddie demolishes the furnishings instead of shooting the villain, strictly in order to prove herself the inept female in need of masculine assistance.   I often feel that Susan Faludi overreacts in seeing male chauvinist conspiracy in simple entertainment, but in this particular case I\\'m all with her - Moonlighting has BACKLASH stamped all over it.   In one sense, however, this DVD is a must for all serious Bruce Willis fans: in addition to the pilot episode, it contains the screen test that landed Willis the job. Both features show to what amazing extent Willis\\' acting ability developed between 1985 and 1988/89 (Die Hard 1, In Country). Impressive!   Rating (and I _am_ a Bruce Willis fan): 2 out of 10'\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 15:58:04.001566: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for text, label in data_train.take(4):\n",
    "    print(text.numpy()[0])\n",
    "    print(label.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e8736-a8f6-4cbf-bf4d-f89e1748b429",
   "metadata": {},
   "source": [
    "<h3>Vamos criar a nossa rede</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0f7b4d-6a7a-46ca-bcaa-2aecca689fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f15086-4fce-403c-88d4-b0335db99f29",
   "metadata": {},
   "source": [
    "<h3>Vamos criar uma camada de vetores de texto.</h3>h3>\n",
    "Dois parâmetros importantes são o tamanho do vocabulário e a dimensionalidade do vetor. Antes de adicionar a camada ao modelo, precisamos adaptar essa camada ao textos que serão usados. No caso de estourar o tamanho máximo do vocabulários, novas palavras serão classificadas como \"out of vocabulary\" 00V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8695863-2506-40e5-878e-f06cdf135a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-04 15:58:07.222550: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 2000  # tamanho do vocabulário\n",
    "max_dim = 300 # dimensionalidade do vetor. No fundo significa que apenas os max_dim primeiros tokens serão usados para converter um texto em números\n",
    "vector_layer = TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_dim)\n",
    "vector_layer.adapt(text_train)\n",
    "model.add(vector_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7447b3f-961f-4864-be6c-b3b43e9ade4b",
   "metadata": {},
   "source": [
    "Vamos ver as palavras mais frequêntes no dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ef9fe0-0bf5-4da6-8e29-fe8b43d54e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  \n",
      "1: [UNK] \n",
      "2: the \n",
      "3: and \n",
      "4: a \n",
      "5: of \n",
      "6: to \n",
      "7: is \n",
      "8: in \n",
      "9: it \n",
      "10: i \n",
      "11: this \n",
      "12: that \n",
      "13: was \n",
      "14: as \n",
      "15: for \n",
      "16: with \n",
      "17: movie \n",
      "18: but \n",
      "19: film \n"
     ]
    }
   ],
   "source": [
    "vocabulary = vector_layer.get_vocabulary()\n",
    "n = 20\n",
    "for index in range(n):\n",
    "    print(f'{index}: {vocabulary[index]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad1cd4-4d41-4aec-b041-2cdae3c4353e",
   "metadata": {},
   "source": [
    "Vamos vetorizar uma frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df25f220-5e58-43a9-a244-b756763b94f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 300), dtype=int64, numpy=\n",
       "array([[  18, 1220,    9,  144,   53,  543,   48,    2,   19,    7,   42,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frase = [[\"But honestly, it doesn't even matter what the film is about.\"]]\n",
    "vetor = vector_layer(frase)\n",
    "vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f7033-7a59-40d3-9df0-a3439620b73e",
   "metadata": {},
   "source": [
    "<h3>Criar uma camada de embedding</h3> Note que o tamanho do vocabulário é max_tokens + 1 por conta do out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a096e808-cbb9-4c54-9052-7db4a0c81c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim = max_tokens + 1, output_dim = 128)\n",
    "model.add(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb326ff-390c-4185-bcc1-c32503755323",
   "metadata": {},
   "source": [
    "Vamos passar o vetor anterior pela camada de embedding e ver o que sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01ce614-1fd2-46e8-a612-5e2b7bdb3d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 300, 128), dtype=float32, numpy=\n",
       "array([[[-0.03481696, -0.0008384 , -0.01511191, ...,  0.02641903,\n",
       "          0.00711516,  0.0178939 ],\n",
       "        [ 0.02262378,  0.01581805, -0.04367347, ..., -0.01852804,\n",
       "         -0.03592256, -0.04343842],\n",
       "        [-0.04170716,  0.01707349, -0.04461306, ...,  0.02186661,\n",
       "          0.00029119, -0.04977271],\n",
       "        ...,\n",
       "        [-0.02977302, -0.0094866 , -0.03631153, ...,  0.02489788,\n",
       "         -0.00381721,  0.02919066],\n",
       "        [-0.02977302, -0.0094866 , -0.03631153, ...,  0.02489788,\n",
       "         -0.00381721,  0.02919066],\n",
       "        [-0.02977302, -0.0094866 , -0.03631153, ...,  0.02489788,\n",
       "         -0.00381721,  0.02919066]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = embedding_layer(vetor)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd2543f-415f-4670-8787-f417d61ae03a",
   "metadata": {},
   "source": [
    "<h3>Vamos adicionar as camadas restantes, sendo uma delas do tipo LSTM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf86138a-9eb3-4ba4-ab5c-962534c2c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61dc74a-f63e-4585-8d9c-756e0541e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37535a0d-97e3-439f-af75-b183d0e69544",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss='binary_crossentropy',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cfbd1df-d09d-4636-9315-193b7b6f26cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 86ms/step - accuracy: 0.5141 - loss: 0.6926 - val_accuracy: 0.5015 - val_loss: 0.6945\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 108ms/step - accuracy: 0.5191 - loss: 0.6872 - val_accuracy: 0.5461 - val_loss: 0.6719\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 113ms/step - accuracy: 0.5517 - loss: 0.6713 - val_accuracy: 0.7133 - val_loss: 0.5902\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 111ms/step - accuracy: 0.7780 - loss: 0.4840 - val_accuracy: 0.8414 - val_loss: 0.3609\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 116ms/step - accuracy: 0.8563 - loss: 0.3441 - val_accuracy: 0.8597 - val_loss: 0.3367\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 107ms/step - accuracy: 0.8884 - loss: 0.2825 - val_accuracy: 0.8636 - val_loss: 0.3409\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 110ms/step - accuracy: 0.9070 - loss: 0.2413 - val_accuracy: 0.8620 - val_loss: 0.3528\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 112ms/step - accuracy: 0.9232 - loss: 0.2066 - val_accuracy: 0.8607 - val_loss: 0.3878\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 119ms/step - accuracy: 0.9334 - loss: 0.1806 - val_accuracy: 0.8532 - val_loss: 0.4311\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 108ms/step - accuracy: 0.9462 - loss: 0.1556 - val_accuracy: 0.8580 - val_loss: 0.4643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f99cd911640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_train, validation_data = data_test, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a819a886-4152-46c6-b5fb-c6ccd9f7b9c5",
   "metadata": {},
   "source": [
    "<h3>Vamos ver como a camada de embedding ficou depois de ser treinada</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efb25be4-979f-465b-bddc-d6351a8fd1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 300, 128), dtype=float32, numpy=\n",
       "array([[[ 0.1831876 , -0.12070046, -0.18899943, ...,  0.2586965 ,\n",
       "         -0.06618347,  0.4126398 ],\n",
       "        [-0.03309989, -0.16083215, -0.31954482, ...,  0.0948881 ,\n",
       "          0.24190028,  0.14767277],\n",
       "        [-0.09027941,  0.16346623,  0.06425152, ..., -0.03135087,\n",
       "         -0.03991551,  0.02266031],\n",
       "        ...,\n",
       "        [ 0.767456  , -0.07145088,  0.76197535, ...,  0.57415056,\n",
       "          0.06219751, -0.03874456],\n",
       "        [ 0.767456  , -0.07145088,  0.76197535, ...,  0.57415056,\n",
       "          0.06219751, -0.03874456],\n",
       "        [ 0.767456  , -0.07145088,  0.76197535, ...,  0.57415056,\n",
       "          0.06219751, -0.03874456]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultado = embedding_layer(vetor)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f6e171-5eb0-4e1f-8568-0d482c98709d",
   "metadata": {},
   "source": [
    "<h3>Salvando o modelo</h3>\n",
    "Treinar um modelo de linguagem leva tempo e custa CPU/GPU. Normalmente você não faz isso o tempo todo. Você treina o seu modelo e salva as saídas. Vamos fazer isso e depois usar esse treinamento em uma outra rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa080f11-6738-4357-8a65-bfbc35306987",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'meu_modelo.keras'\n",
    "model.save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b238c64-d5ba-41de-b6fb-b013e3699d79",
   "metadata": {},
   "source": [
    "<h3>Vamos criar um novo modelo e carregar esse treinamento para usar</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b3d1634-4125-4a5c-8614-854503c24406",
   "metadata": {},
   "outputs": [],
   "source": [
    "uso = load_model(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80903427-4aa2-429f-8874-3676ec66379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_review = [\"\"\"I had already watched this movie, but I remembered almost nothing, including that several interesting actors are part of it. \n",
    "The opening scene is intensely gory and very interesting, with crude visual effects worthy of 2002, but very pleasing to the eyes of fans of B movies from the 90s/00s. \n",
    "Without a doubt, it is a film worth watching and rewatching, especially if the viewer enjoys supernatural exploitation films.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65701637-7089-4617-a6a1-04fa00cd65ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
      "[[0.01852844]]\n"
     ]
    }
   ],
   "source": [
    "print(uso.predict(convert_to_tensor(meu_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc9170b0-d367-47b2-9edb-90f2c3fc7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_review = [\"\"\"It has no plot, no comedy, no drama, no passion. You basically waste 2 hours of your time watching these characters you just can't seem to get attached to... \n",
    "Watch it only if you're interested in watching all these actors as they were before they got famous, or if you feel REALLY nostalgic about the '80s.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0bcd930-39b2-4745-bbee-d8f0e5167cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "[[0.00242021]]\n"
     ]
    }
   ],
   "source": [
    "print(uso.predict(convert_to_tensor(meu_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "748d3a96-a672-4968-9273-8031576b2266",
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_review = [\"\"\"I usually steer toward Sci-Fi and Fantasy movies, but after watching the The Breakfast Club, I thought I may enjoy seeing actors and actresses like Judd Nelson, and Ally Sheedy working together again. \n",
    "Well, I was right. Some movies can make you feel different emotions by being tearful, or violent. St.Elmo's Fire made me cry- not because of the actual plot, but the way it truly played to real life. \n",
    "The aspects of this movie letf wishing to see what would happen next, after the character's old lives were left behind.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855f1e60-2f10-431d-9970-26d8ebe1def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "[[0.99375916]]\n"
     ]
    }
   ],
   "source": [
    "print(uso.predict(convert_to_tensor(meu_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e84dde-ff13-42bd-8779-eb34cbe7b036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4737742-74dc-4db6-8a55-4d9f6a00f1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
